#!/usr/bin/env python3
"""
Test Ollama Cloud Models

Test script for Ollama cloud models including Kimi K2, DeepSeek V3, GPT-OSS, etc.
"""

import os
import sys
import json
from pathlib import Path

# Ollama credentials
OLLAMA_KEY = "f1f55b29520c411ca42370b8d9619ea0.TbznNbrG3PhFSNfpb0GQOrj-"

# Available models
OLLAMA_MODELS = [
    "deepseek-v3.1:671b-cloud",
    "gpt-oss:20b-cloud",
    "gpt-oss:120b-cloud",
    "kimi-k2:1t-cloud",  # Kimi K2 - great for reasoning!
    "qwen3-coder:480b-cloud",
    "glm-4.6:cloud",
    "minimax-m2:cloud",
]


def test_ollama_basic():
    """Test basic Ollama connection."""
    try:
        from ollama import Client
    except ImportError:
        print("‚ùå Ollama not installed. Installing...")
        import subprocess
        subprocess.check_call([sys.executable, "-m", "pip", "install", "ollama"])
        from ollama import Client

    print("üîå Connecting to Ollama Cloud...")
    client = Client(
        host="https://ollama.com",
        headers={'Authorization': f'Bearer {OLLAMA_KEY}'}
    )

    print("‚úì Connected!")
    return client


def test_model(client, model_name: str, prompt: str = "Explain reinforcement learning in one sentence."):
    """Test a specific model."""
    print(f"\n{'='*60}")
    print(f"Testing: {model_name}")
    print('='*60)

    messages = [
        {
            'role': 'user',
            'content': prompt,
        },
    ]

    print(f"Prompt: {prompt}\n")
    print("Response: ", end="", flush=True)

    full_response = ""
    try:
        for part in client.chat(model_name, messages=messages, stream=True):
            text = part['message']['content']
            print(text, end='', flush=True)
            full_response += text
        print("\n")
        return full_response
    except Exception as e:
        print(f"\n‚ùå Error: {e}")
        return None


def generate_blog_post_with_kimi(client, paper_data: dict):
    """Generate a blog post using Kimi K2."""
    print(f"\n{'='*60}")
    print("üöÄ Generating Blog Post with Kimi K2")
    print('='*60 + "\n")

    system_prompt = """You are a technical writer specializing in AI and machine learning.

Writing style: clear, friendly, explains concepts simply
Target length: ~800 words
Format: Blog post with:
- Compelling title
- Hook/introduction
- Clear explanation of problem
- Technical details (accessible to developers)
- Key insights
- Practical implications
- Conclusion

Use markdown formatting."""

    user_prompt = f"""Write a blog post about this paper:

**Title**: {paper_data['title']}
**Authors**: {', '.join(paper_data['authors'][:3])}
**Published**: {paper_data['published']}

**Abstract**:
{paper_data['summary'][:500]}...

Create an engaging blog post explaining this research, why it matters, and what developers should know."""

    messages = [
        {'role': 'system', 'content': system_prompt},
        {'role': 'user', 'content': user_prompt}
    ]

    print("ü§ñ Kimi K2 is thinking and writing...\n")
    print("="*60 + "\n")

    full_response = ""
    for part in client.chat('kimi-k2:1t-cloud', messages=messages, stream=True):
        text = part['message']['content']
        print(text, end='', flush=True)
        full_response += text

    print("\n" + "="*60)

    # Save output
    output_dir = Path(__file__).parent.parent / "outputs" / "blogs"
    output_dir.mkdir(parents=True, exist_ok=True)

    output_file = output_dir / f"kimi_k2_test_{paper_data['arxiv_id'].replace('/', '-')}.md"
    with open(output_file, 'w') as f:
        f.write(f"# Generated by Kimi K2\n\n")
        f.write(full_response)

    print(f"\nüíæ Saved to: {output_file}")
    return full_response


def generate_twitter_thread_with_deepseek(client, paper_data: dict):
    """Generate Twitter thread using DeepSeek V3."""
    print(f"\n{'='*60}")
    print("üöÄ Generating Twitter Thread with DeepSeek V3")
    print('='*60 + "\n")

    prompt = f"""Create a Twitter thread (max 8 tweets) about this AI research paper:

Title: {paper_data['title']}
Summary: {paper_data['summary'][:400]}...

Make it:
- Engaging and accessible
- Each tweet max 280 characters
- Start with a hook
- End with a takeaway

Format: Number each tweet (1/, 2/, etc.)"""

    messages = [{'role': 'user', 'content': prompt}]

    print("ü§ñ DeepSeek V3 is creating your thread...\n")
    print("="*60 + "\n")

    full_response = ""
    for part in client.chat('deepseek-v3.1:671b-cloud', messages=messages, stream=True):
        text = part['message']['content']
        print(text, end='', flush=True)
        full_response += text

    print("\n" + "="*60)
    return full_response


def main():
    print("="*60)
    print("üß™ Ollama Cloud Models Test")
    print("="*60 + "\n")

    # Test connection
    client = test_ollama_basic()

    # Quick test with each model
    print("\n" + "="*60)
    print("Quick Test of All Models")
    print("="*60)

    for model in ["kimi-k2:1t-cloud", "deepseek-v3.1:671b-cloud", "gpt-oss:120b-cloud"]:
        test_model(client, model, "What is RLHF in one sentence?")

    # Load a paper from the database
    print("\n" + "="*60)
    print("Loading Paper from Database")
    print("="*60)

    db_path = Path(__file__).parent.parent.parent / "scripts" / "papers_database.json"
    with open(db_path, 'r') as f:
        database = json.load(f)

    # Get first paper about RLHF
    sample_paper = database['llm_alignment'][0]
    print(f"\nSelected paper: {sample_paper['title'][:60]}...")

    # Generate blog post with Kimi K2
    blog = generate_blog_post_with_kimi(client, sample_paper)

    # Generate thread with DeepSeek V3
    thread = generate_twitter_thread_with_deepseek(client, sample_paper)

    print("\n" + "="*60)
    print("‚úÖ All tests complete!")
    print("="*60)
    print("\nKey takeaways:")
    print("- Kimi K2 (kimi-k2:1t-cloud): Excellent for detailed content")
    print("- DeepSeek V3 (deepseek-v3.1:671b-cloud): Fast and capable")
    print("- GPT-OSS (gpt-oss:120b-cloud): Good alternative")
    print("- Qwen3 Coder (qwen3-coder:480b-cloud): Great for code content")
    print("\nCheck outputs/ directory for generated content!")


if __name__ == "__main__":
    main()
